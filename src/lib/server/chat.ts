import { OPENROUTER_BASE_URL, OPENROUTER_API_KEY, OPENROUTER_MODEL, PROXY_URI } from '$env/static/private';
import OpenAI from 'openai';
import type { SummaryData, ChatMessage } from '$lib/types';
import * as undici from 'undici';
import { getTranscriptByVideoId } from './database.js';

// æµå¼èŠå¤©å“åº”çš„äº‹ä»¶å‘å°„å™¨æ¥å£
export interface ChatStreamEmitters {
	onDelta?: (delta: string) => void;
	onComplete?: (fullResponse: string) => void;
	onError?: (error: string) => void;
}

// Only create proxy agent if PROXY_URI is available
const proxyAgent = PROXY_URI ? new undici.ProxyAgent(PROXY_URI) : null;

const openai = new OpenAI({
	baseURL: OPENROUTER_BASE_URL,
	apiKey: OPENROUTER_API_KEY,
	defaultHeaders: {
		'HTTP-Referer': 'https://gisttube.com',
		'X-Title': 'gisttube',
	},
	fetchOptions: {
		dispatcher: proxyAgent ? proxyAgent : undefined,
	},
});

const chatSystemPrompt = `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºç”¨æˆ·æä¾›çš„è§†é¢‘ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·å…³äºè§†é¢‘å†…å®¹çš„é—®é¢˜ã€‚

## ä½ çš„èƒ½åŠ›
- åˆ†æè§†é¢‘æ ‡é¢˜ã€æè¿°ã€æ€»ç»“å’Œå…³é”®ç‚¹
- åŸºäºåŸå§‹å­—å¹•å†…å®¹è¿›è¡Œæ·±å…¥åˆ†æ
- å›ç­”å…³äºè§†é¢‘å†…å®¹çš„å…·ä½“é—®é¢˜
- æä¾›æ·±å…¥çš„è§£é‡Šå’Œåˆ†æ
- æå–è§†é¢‘ä¸­çš„é‡è¦ä¿¡æ¯
- å¼•ç”¨è§†é¢‘ä¸­çš„å…·ä½“å†…å®¹

## å¯ç”¨ä¿¡æ¯
ä½ å°†è·å¾—ä»¥ä¸‹ä¿¡æ¯ï¼š
1. **è§†é¢‘åŸºæœ¬ä¿¡æ¯**: æ ‡é¢˜ã€ä½œè€…ã€æè¿°
2. **AIç”Ÿæˆçš„æ€»ç»“**: è§†é¢‘æ€»ç»“ã€å…³é”®è¦ç‚¹ã€å…³é”®ç‚¹åˆ—è¡¨ã€æ ¸å¿ƒæœ¯è¯­
3. **åŸå§‹å­—å¹•å†…å®¹**: è§†é¢‘çš„å®Œæ•´å­—å¹•æ–‡æœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚å­—å¹•æ ¼å¼ä¸º `[æ—¶é—´æˆ³] æ–‡æœ¬å†…å®¹`ï¼Œä¾‹å¦‚ `[00:01:23] è¿™æ˜¯å­—å¹•å†…å®¹`ã€‚æ—¶é—´æˆ³è¡¨ç¤ºè¯¥æ®µå­—å¹•åœ¨è§†é¢‘ä¸­çš„ä½ç½®ã€‚

## å›ç­”åŸåˆ™
1. **åŸºäºè§†é¢‘å†…å®¹**: åªå›ç­”ä¸è§†é¢‘å†…å®¹ç›¸å…³çš„é—®é¢˜
2. **å‡†ç¡®å¯é **: åŸºäºæä¾›çš„è§†é¢‘ä¿¡æ¯è¿›è¡Œå›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
3. **å¼•ç”¨åŸæ–‡**: å¯ä»¥å¼•ç”¨åŸå§‹å­—å¹•ä¸­çš„å…·ä½“å†…å®¹æ¥æ”¯æŒä½ çš„å›ç­”ã€‚å¦‚æœç”¨æˆ·è¯¢é—®ç‰¹å®šæ—¶é—´ç‚¹çš„å†…å®¹ï¼Œå¯ä»¥ä½¿ç”¨å­—å¹•ä¸­çš„æ—¶é—´æˆ³æ¥å®šä½
4. **ç®€æ´æ˜äº†**: å›ç­”è¦æ¸…æ™°ã€æœ‰æ¡ç†
5. **ä¸­æ–‡å›ç­”**: ä½¿ç”¨ç®€ä½“ä¸­æ–‡å›ç­”
6. **è¯šå®**: å¦‚æœè§†é¢‘ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®è¯´æ˜

## å›ç­”æ ¼å¼
- ä½¿ç”¨è‡ªç„¶çš„ä¸­æ–‡è¡¨è¾¾
- å¯ä»¥é€‚å½“ä½¿ç”¨**ç²—ä½“**å¼ºè°ƒé‡è¦å†…å®¹
- å¦‚æœæ¶‰åŠå¤šä¸ªè¦ç‚¹ï¼Œä½¿ç”¨åˆ—è¡¨å½¢å¼
- å¯ä»¥å¼•ç”¨è§†é¢‘ä¸­çš„å…·ä½“å†…å®¹
- ä¿æŒå‹å¥½å’Œä¸“ä¸šçš„è¯­è°ƒ`;

export const generateChatResponse = async (
	userMessage: string,
	videoId: string,
	videoTitle: string,
	summaryData: SummaryData,
	conversationHistory?: ChatMessage[]
): Promise<string> => {
	try {
		// ä»æ•°æ®åº“è·å–åŸå§‹å­—å¹•
		const transcript = await getTranscriptByVideoId(videoId);
		
		// æ„å»ºè§†é¢‘ä¸Šä¸‹æ–‡ä¿¡æ¯
		const videoContext = `
è§†é¢‘æ ‡é¢˜: ${videoTitle}
è§†é¢‘ID: ${videoId}
è§†é¢‘ä½œè€…: ${summaryData.author}
è§†é¢‘æè¿°: ${summaryData.description}

è§†é¢‘æ€»ç»“:
${summaryData.summary}

å…³é”®è¦ç‚¹:
${summaryData.keyTakeaway}

å…³é”®ç‚¹åˆ—è¡¨:
${summaryData.keyPoints?.join('\n') || 'æ— '}

æ ¸å¿ƒæœ¯è¯­:
${summaryData.coreTerms?.join(', ') || 'æ— '}

${summaryData.commentsSummary ? `è§‚ä¼—è¯„è®ºæ€»ç»“:
${summaryData.commentsSummary}

è§‚ä¼—å…³æ³¨è¦ç‚¹:
${summaryData.commentsKeyPoints?.join('\n') || 'æ— '}

è¯„è®ºæ•°é‡: ${summaryData.commentsCount || 0} æ¡` : 'æ³¨æ„: è¯¥è§†é¢‘æš‚æ— è§‚ä¼—è¯„è®ºæ•°æ®'}

${transcript ? `åŸå§‹å­—å¹•å†…å®¹:
${transcript}` : 'æ³¨æ„: è¯¥è§†é¢‘æ²¡æœ‰å¯ç”¨çš„å­—å¹•å†…å®¹'}
		`.trim();

		// æ„å»ºæ¶ˆæ¯æ•°ç»„ï¼ŒåŒ…å«å¯¹è¯å†å²
		const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
			{
				role: "system",
				content: chatSystemPrompt
			}
		];

		// å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯æ•°ç»„ä¸­
		if (conversationHistory && conversationHistory.length > 0) {
			// æ·»åŠ å¯¹è¯å†å²ï¼ˆé™åˆ¶æœ€è¿‘10è½®å¯¹è¯ä»¥é¿å…tokenè¿‡å¤šï¼‰
			const recentHistory = conversationHistory.slice(-10);
			for (const msg of recentHistory) {
				messages.push({
					role: msg.role as "user" | "assistant",
					content: msg.content
				});
			}
		}

		// æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
		messages.push({
			role: "user",
			content: `åŸºäºä»¥ä¸‹è§†é¢‘ä¿¡æ¯ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

è§†é¢‘ä¿¡æ¯:
${videoContext}

ç”¨æˆ·é—®é¢˜: ${userMessage}

è¯·åŸºäºè§†é¢‘å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœé—®é¢˜ä¸è§†é¢‘å†…å®¹æ— å…³ï¼Œè¯·ç¤¼è²Œåœ°è¯´æ˜ä½ åªèƒ½å›ç­”å…³äºè¿™ä¸ªè§†é¢‘çš„é—®é¢˜ã€‚`
		});

		const response = await openai.chat.completions.create({
			model: OPENROUTER_MODEL,
			messages,
		});

		const content = response.choices[0].message.content;
		if (!content) {
			throw new Error('No content received from AI');
		}

		return content;
	} catch (error) {
		console.error('Failed to generate chat response:', error);
		throw new Error('Failed to generate AI response.');
	}
};

/**
 * ç”Ÿæˆæµå¼èŠå¤©å“åº”
 * æ”¯æŒå®æ—¶æµå¼è¾“å‡ºAIå›ç­”
 */
export const generateChatResponseStream = async (
	userMessage: string,
	videoId: string,
	videoTitle: string,
	summaryData: SummaryData,
	conversationHistory?: ChatMessage[],
	emitters: ChatStreamEmitters = {}
): Promise<string> => {
	const startTime = Date.now();
	let llmRequestStart = 0;
	let firstTokenTime = 0;
	let llmFirstResponseTime = 0;
	let fullResponse = '';

	try {
		console.log(`[chat-stream] ğŸš€ Starting streaming chat for video ${videoId}`);
		
		// ä»æ•°æ®åº“è·å–åŸå§‹å­—å¹•
		const transcript = await getTranscriptByVideoId(videoId);
		
		// æ„å»ºè§†é¢‘ä¸Šä¸‹æ–‡ä¿¡æ¯
		const videoContext = `
è§†é¢‘æ ‡é¢˜: ${videoTitle}
è§†é¢‘ID: ${videoId}
è§†é¢‘ä½œè€…: ${summaryData.author}
è§†é¢‘æè¿°: ${summaryData.description}

è§†é¢‘æ€»ç»“:
${summaryData.summary}

å…³é”®è¦ç‚¹:
${summaryData.keyTakeaway}

å…³é”®ç‚¹åˆ—è¡¨:
${summaryData.keyPoints?.join('\n') || 'æ— '}

æ ¸å¿ƒæœ¯è¯­:
${summaryData.coreTerms?.join(', ') || 'æ— '}

${summaryData.commentsSummary ? `è§‚ä¼—è¯„è®ºæ€»ç»“:
${summaryData.commentsSummary}

è§‚ä¼—å…³æ³¨è¦ç‚¹:
${summaryData.commentsKeyPoints?.join('\n') || 'æ— '}

è¯„è®ºæ•°é‡: ${summaryData.commentsCount || 0} æ¡` : 'æ³¨æ„: è¯¥è§†é¢‘æš‚æ— è§‚ä¼—è¯„è®ºæ•°æ®'}

${transcript ? `åŸå§‹å­—å¹•å†…å®¹:
${transcript}` : 'æ³¨æ„: è¯¥è§†é¢‘æ²¡æœ‰å¯ç”¨çš„å­—å¹•å†…å®¹'}
		`.trim();

		// æ„å»ºæ¶ˆæ¯æ•°ç»„ï¼ŒåŒ…å«å¯¹è¯å†å²
		const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
			{
				role: "system",
				content: chatSystemPrompt
			}
		];

		// å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯æ•°ç»„ä¸­
		if (conversationHistory && conversationHistory.length > 0) {
			// æ·»åŠ å¯¹è¯å†å²ï¼ˆé™åˆ¶æœ€è¿‘10è½®å¯¹è¯ä»¥é¿å…tokenè¿‡å¤šï¼‰
			const recentHistory = conversationHistory.slice(-10);
			for (const msg of recentHistory) {
				messages.push({
					role: msg.role as "user" | "assistant",
					content: msg.content
				});
			}
		}

		// æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
		messages.push({
			role: "user",
			content: `åŸºäºä»¥ä¸‹è§†é¢‘ä¿¡æ¯ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

è§†é¢‘ä¿¡æ¯:
${videoContext}

ç”¨æˆ·é—®é¢˜: ${userMessage}

è¯·åŸºäºè§†é¢‘å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœé—®é¢˜ä¸è§†é¢‘å†…å®¹æ— å…³ï¼Œè¯·ç¤¼è²Œåœ°è¯´æ˜ä½ åªèƒ½å›ç­”å…³äºè¿™ä¸ªè§†é¢‘çš„é—®é¢˜ã€‚`
		});

		llmRequestStart = Date.now();
		console.log(`ğŸ“Š Chat ${videoId} - LLM request initiated at ${llmRequestStart}`);

		// åˆ›å»ºæµå¼è¯·æ±‚
		const stream = await openai.chat.completions.create({
			model: OPENROUTER_MODEL,
			messages,
			stream: true,
		});

		// å¤„ç†æµå¼å“åº”
		for await (const chunk of stream) {
			// è®°å½•ç¬¬ä¸€ä¸ªtokenåˆ°è¾¾æ—¶é—´
			if (firstTokenTime === 0) {
				firstTokenTime = Date.now();
				llmFirstResponseTime = firstTokenTime - llmRequestStart;
				console.log(`ğŸ“Š Chat ${videoId} - First token received: ${llmFirstResponseTime}ms after request`);
			}

			const delta = chunk.choices[0]?.delta?.content || '';
			if (delta) {
				fullResponse += delta;
				emitters.onDelta?.(delta);
			}
		}

		const totalTime = Date.now() - startTime;
		console.log(`ğŸ‰ Streaming chat completed for ${videoId} in ${totalTime}ms`, {
			llmTiming: {
				requestToFirstToken: llmFirstResponseTime,
				totalGenerationTime: Date.now() - llmRequestStart,
				tokensPerSecond: llmFirstResponseTime > 0 ? Math.round(1000 / llmFirstResponseTime * 100) / 100 : 0
			},
			responseLength: fullResponse.length,
			hasTranscript: !!transcript,
			conversationHistoryLength: conversationHistory?.length || 0
		});

		emitters.onComplete?.(fullResponse);
		return fullResponse;

	} catch (error) {
		console.error('Failed to generate streaming chat response:', error);
		const errorMessage = 'æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›å¤ã€‚è¯·ç¨åå†è¯•ã€‚';
		emitters.onError?.(errorMessage);
		throw new Error('Failed to generate streaming AI response.');
	}
};
